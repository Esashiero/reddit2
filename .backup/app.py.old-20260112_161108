from flask import Flask, Response, request, render_template, jsonify
import os
import praw
import json
import time
import requests
import math
import re
import argparse
import sys
from typing import List, Dict, Optional, Generator
from dotenv import load_dotenv, find_dotenv
from mistralai import Mistral
import prawcore

load_dotenv(find_dotenv(), override=True)

SAVED_QUERIES_FILE = "saved_queries.json"
SUBREDDITS_FILE = "subreddits.json"
BLACKLIST_FILE = "blacklist.json"

DEFAULT_SUBS_ENV = os.getenv(
    "REDDIT_SUBREDDITS", "ArtificialInteligence,LocalLLaMA,Python"
)
REDDIT_ID = os.getenv("REDDIT_CLIENT_ID")
REDDIT_SECRET = os.getenv("REDDIT_CLIENT_SECRET")
REDDIT_AGENT = os.getenv("REDDIT_USER_AGENT", "script:flask_app:v12_fixed")
MISTRAL_KEY = os.getenv("MISTRAL_API_KEY", "")
GOOGLE_KEY = os.getenv("GOOGLE_API_KEY", "")


class LLMFilter:
    def __init__(self, provider: str):
        self.provider = provider
        if provider == "mistral":
            if not MISTRAL_KEY:
                raise Exception("Mistral Key Missing")
            self.client = Mistral(api_key=MISTRAL_KEY)
        elif provider == "gemini":
            if not GOOGLE_KEY:
                raise Exception("Google Key Missing")
            self.api_key = GOOGLE_KEY

    def analyze(self, posts: List[Dict], criteria: str) -> List[Dict]:
        sys_p = (
            "Analyze the following Reddit posts based on the criteria. "
            "Return a JSON object with a 'results' key containing a list of objects. "
            "Each object must have 'id' and 'score' (0-100, where 100 is perfect match). "
            'Example: {"results": [{"id": "abc", "score": 85}]}'
        )
        usr_p = f"CRITERIA: {criteria}\nPOSTS:\n{json.dumps(posts)}"
        
        retries = 3
        delay = 5
        while retries > 0:
            try:
                if self.provider == "mistral":
                    res = self.client.chat.complete(
                        model="mistral-small-latest",
                        messages=[
                            {"role": "system", "content": sys_p},
                            {"role": "user", "content": usr_p},
                        ],
                        response_format={"type": "json_object"},
                    )
                    return json.loads(str(res.choices[0].message.content)).get(
                        "results", []
                    )
                else:
                    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={self.api_key}"
                    pl = {
                        "contents": [{"parts": [{"text": usr_p}]}],
                        "system_instruction": {"parts": [{"text": sys_p}]},
                        "generationConfig": {"response_mime_type": "application/json"},
                    }
                    res = requests.post(
                        url, json=pl, headers={"Content-Type": "application/json"}
                    )
                    txt = res.json()["candidates"][0]["content"]["parts"][0]["text"]
                    return json.loads(txt).get("results", [])
            except Exception as e:
                if "429" in str(e):
                    print(f"LLM rate limit hit. Retrying in {delay}s...", file=sys.stderr)
                    time.sleep(delay)
                    retries -= 1
                    delay *= 2
                else:
                    print(f"LLM Error: {e}", file=sys.stderr)
                    return []
        return []

    def expand_query(self, original_keywords: str, criteria: str) -> str:
        sys_p = (
            "You are an expert Reddit Search Engineer. The user's search returned 0 results. "
            "Suggest a broader but still relevant Boolean Search String. "
            "Format: (term1 OR term2) AND (termA OR termB). "
            "Return ONLY the raw string."
        )
        usr_p = f"Original Keywords: {original_keywords}\nCriteria: {criteria}"
        
        retries = 3
        delay = 5
        while retries > 0:
            try:
                if self.provider == "mistral":
                    res = self.client.chat.complete(
                        model="mistral-large-latest",
                        messages=[
                            {"role": "system", "content": sys_p},
                            {"role": "user", "content": usr_p},
                        ],
                    )
                    content = res.choices[0].message.content
                    return str(content).strip() if content else original_keywords
                else:
                    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?key={self.api_key}"
                    pl = {
                        "contents": [{"parts": [{"text": usr_p}]}],
                        "system_instruction": {"parts": [{"text": sys_p}]},
                    }
                    res = requests.post(
                        url, json=pl, headers={"Content-Type": "application/json"}
                    )
                    return res.json()["candidates"][0]["content"]["parts"][0][
                        "text"
                    ].strip()
            except Exception as e:
                if "429" in str(e):
                    print(f"LLM rate limit hit. Retrying in {delay}s...", file=sys.stderr)
                    time.sleep(delay)
                    retries -= 1
                    delay *= 2
                else:
                    print(f"LLM Error: {e}", file=sys.stderr)
                    return original_keywords
        return original_keywords

    def generate_boolean_string(self, description: str) -> str:
        sys_p = (
            "You are an expert Reddit Search Engineer. Your task is to generate a high-precision Boolean Search String "
            "based on the user's description. The query must be optimized for Reddit's search engine.\n\n"
            "**SOCRATIC INTENT DECOMPOSITION:**\n"
            "Before generating the query, you MUST perform a 3-axis decomposition:\n"
            "1. **Clarification**: What is the core event or entity being sought?\n"
            "2. **Assumption Probing**: What implicit details must be present (e.g., gender, relationship, specific setting)?\n"
            "3. **Implication Probing**: What linguistic markers or 'aftermath jargon' would a user naturally use when discussing this?\n\n"
            "**CRITICAL PRINCIPLE:** Precision over recall. Better to return 5-30 highly relevant results than 100 irrelevant ones.\n\n"
            "**LUCENE OPERATOR STANDARDS:**\n"
            "1. **Proximity Search:** Use `~10` for conversational stories and `~5` for technical/logical concepts.\n"
            "2. **Boosting:** Use `^N` (e.g., `^2`) to prioritize core entities or mandatory terms.\n"
            "3. **Field Filters:** Use `self:true` to force text-only posts and `title:` for specific title matches.\n\n"
            "**Rules:**\n"
            "1. **Structure:** Identify ONLY 2-3 MOST critical concepts. Each additional AND group dramatically reduces results. Connect groups with AND.\n"
            "2. **EXTRACTION STRATEGY:**\n"
            "   - Focus on distinctive, high-signal entities and actions\n"
            "   - For each critical concept, include 3-4 natural variations people use on Reddit\n"
            "3. **SPECIFICITY RULES:**\n"
            "   - NEVER use generic story terms: 'story', 'experience', 'personal', 'happened to me', 'first person', 'true story'\n"
            '5. **Format:** Return ONLY the raw string: (term1 OR term2 OR "term 3") AND (term4 OR term5 OR term6).\n'
            "6. **Constraints:** No markdown, no explanations. Do not nest parentheses."
        )
        
        retries = 3
        delay = 5
        while retries > 0:
            try:
                if self.provider == "mistral":
                    res = self.client.chat.complete(
                        model="mistral-large-latest",
                        messages=[
                            {"role": "system", "content": sys_p},
                            {"role": "user", "content": description},
                        ],
                    )
                    content = res.choices[0].message.content
                    query_string = str(content).strip() if content else ""
                else:
                    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?key={self.api_key}"
                    pl = {
                        "contents": [{"parts": [{"text": description}]}],
                        "system_instruction": {"parts": [{"text": sys_p}]},
                    }
                    res = requests.post(
                        url, json=pl, headers={"Content-Type": "application/json"}
                    )
                    response_json = res.json()
                    if "candidates" not in response_json:
                        return f"Error: Gemini API response missing candidates. Full response: {json.dumps(response_json)}"
                    query_string = response_json["candidates"][0]["content"]["parts"][0][
                        "text"
                    ].strip()
                query_string = query_string.replace("```", "").strip()
                if query_string.startswith('"') and query_string.endswith('"'):
                    query_string = query_string[1:-1]
                return query_string
            except Exception as e:
                if "429" in str(e):
                    print(f"LLM rate limit hit. Retrying in {delay}s...", file=sys.stderr)
                    time.sleep(delay)
                    retries -= 1
                    delay *= 2
                else:
                    return f"Error: {e}"
        return "Error: LLM rate limit exceeded after multiple retries."


DEFAULT_SEARCH_SUBREDDITS = [
    "offmychest",
    "sexualassault",
    "TwoXChromosomes",
    "amiwrong",
    "relationship_advice",
    "confessions",
    "india",
    "indiasocial",
    "advice",
    "relationships",
    "lifeadvice",
    "trueoffmychest",
    "self",
    "rapecounseling",
    "vent",
    "questions",
    "familyissues",
    "family",
]


class RedditSearchEngine:
    def __init__(self, provider: str = "mistral", debug: bool = False):
        self.provider = provider
        self.debug = debug
        self.reddit = praw.Reddit(
            client_id=REDDIT_ID, client_secret=REDDIT_SECRET, user_agent=REDDIT_AGENT
        )
        self.llm = LLMFilter(provider)

    def _log(self, msg: str):
        if self.debug:
            print(f"DEBUG: {msg}", file=sys.stderr)

    @staticmethod
    def parse_keywords(keywords: str) -> List[List[str]]:
        parsed_groups, paren_groups = [], re.findall(r"\(([^)]+)\)", keywords)
        for g in paren_groups:
            terms = re.split(r"\s+OR\s+|\s+\|\s+", g, flags=re.IGNORECASE)
            clean_terms = [t.strip() for t in terms if t.strip()]
            if clean_terms:
                parsed_groups.append(clean_terms)
        remaining = re.sub(r"\(([^)]+)\)", "", keywords).strip()
        remaining = re.sub(r"^(AND|OR)\s+", "", remaining, flags=re.IGNORECASE)
        if remaining:
            parts = re.split(r"\s+AND\s+", remaining, flags=re.IGNORECASE)
            for p in parts:
                if p.strip():
                    parsed_groups.append([p.strip()])
        return parsed_groups

    @staticmethod
    def build_reddit_query(parsed_groups: List[List[str]]) -> str:
        if not parsed_groups:
            return ""
        and_blocks = []
        for group in parsed_groups:
            or_terms = [
                term
                if ":" in term
                or "~" in term
                or "^" in term
                or (term.startswith('"') and term.endswith('"'))
                else f'"{term}"'
                for term in group
            ]
            block = (
                "(" + " OR ".join(or_terms) + ")" if len(or_terms) > 1 else or_terms[0]
            )
            and_blocks.append(block)
        return " AND ".join(and_blocks)

    @staticmethod
    def matches_logic(text: str, parsed_groups: List[List[str]]) -> bool:
        if not parsed_groups:
            return True
        text_lower = text.lower()
        for group in parsed_groups:
            clean_group = [t.replace('"', "") for t in group]
            if not any(t in text_lower for t in clean_group):
                return False
        return True

    def search(
        self,
        keywords: str,
        criteria: str,
        subreddits: List[str],
        target_posts: int = 10,
        ai_chunk_size: int = 5,
        is_deep_scan: bool = False,
        manual_limit: int = 100,
        sort_by: str = "new",
        time_filter: str = "all",
        post_type: str = "any",
    ) -> Generator[str, None, List]:
        parsed_groups = self.parse_keywords(keywords)
        reddit_query = self.build_reddit_query(parsed_groups)
        yield f"‚öôÔ∏è Mode: Iterative Search\nüîé Query: {reddit_query}\nüéØ Goal: Find {target_posts} posts total."

        all_matches = []

        def perform_fetch(query, limit, start_time=None, end_time=None):
            final_query = (
                f"{query} timestamp:{start_time}..{end_time}"
                if start_time and end_time
                else query
            )

            search_subs = subreddits
            if "all" in search_subs:
                search_subs = DEFAULT_SEARCH_SUBREDDITS
            
            yield f"‚ÑπÔ∏è Searching across {len(search_subs)} subreddits."

            for sub in search_subs:
                retries = 3
                delay = 5
                while retries > 0:
                    try:
                        for post in self.reddit.subreddit(sub).search(
                            final_query, sort=sort_by, time_filter=time_filter, limit=limit
                        ):
                            if (post_type == "text" and not post.is_self) or (
                                post_type == "media" and post.is_self
                            ):
                                continue
                            if self.matches_logic(
                                f"{post.title} {post.selftext}", parsed_groups
                            ):
                                yield post
                        break  # Success
                    except prawcore.exceptions.Forbidden:
                        self._log(f"Warning: r/{sub} is private or banned. Skipping.")
                        break  # Skip this subreddit
                    except prawcore.exceptions.RequestException as e:
                        self._log(f"Error r/{sub}: {e}. Retrying in {delay}s...")
                        time.sleep(delay)
                        retries -= 1
                        delay *= 2
                    except Exception as e:
                        self._log(f"An unexpected error occurred for r/{sub}: {e}")
                        break

        if is_deep_scan:
            yield "üïí Deep Scan: Implementing Chronological Sliding Windows (30-day batches)..."
            WINDOW_SIZE, end_ts = 30 * 24 * 60 * 60, int(time.time())
            for i in range(24):
                start_ts = end_ts - WINDOW_SIZE
                yield f"   üìÖ Window {i + 1}: {time.strftime('%Y-%m-%d', time.gmtime(start_ts))} to {time.strftime('%Y-%m-%d', time.gmtime(end_ts))}"
                window_matches = []
                for item in perform_fetch(reddit_query, 100, start_ts, end_ts):
                    if isinstance(item, str):
                        yield item
                    else:
                        window_matches.append(item)
                existing_ids = {p.id for p in all_matches}
                new_matches = [p for p in window_matches if p.id not in existing_ids]
                all_matches.extend(new_matches)
                yield f"      Found {len(new_matches)} new candidates (Total: {len(all_matches)})"
                if len(all_matches) >= 500:
                    break
                end_ts, _ = start_ts, time.sleep(1)
        else:
            for item in perform_fetch(reddit_query, manual_limit):
                if isinstance(item, str):
                    yield item
                else:
                    all_matches.append(item)
        if not all_matches and criteria:
            yield "üîÑ No results. Attempting query expansion..."
            expanded_keywords = self.llm.expand_query(keywords, criteria)
            yield f"üîé Expanded Query: {expanded_keywords}"
            fetch_result = perform_fetch(
                self.build_reddit_query(self.parse_keywords(expanded_keywords)),
                manual_limit,
            )
            for item in fetch_result:
                if isinstance(item, str):
                    yield item
                else:
                    all_matches.append(item)

        if not all_matches:
            yield "\n‚ùå No posts found."
            return []

        yield f"‚úÖ Found {len(all_matches)} initial candidate posts."
        final_posts = all_matches[:target_posts]
        if criteria:
            yield f"\nü§ñ AI Analyzing {len(all_matches)} candidates..."
            processed, pmap = [], {}
            for p in all_matches:
                body = (
                    p.selftext[:30000] + "..."
                    if len(p.selftext) > 30000
                    else p.selftext or "[No Body]"
                )
                processed.append(
                    {
                        "id": p.id,
                        "sub": p.subreddit.display_name,
                        "title": p.title,
                        "content": body,
                    }
                )
                pmap[p.id] = p

            scored_results = []
            for i in range(0, len(processed), ai_chunk_size):
                batch = processed[i : i + ai_chunk_size]
                yield f"   Chunk {i // ai_chunk_size + 1}..."
                results = self.llm.analyze(batch, criteria)
                if not results:
                    yield "   ‚ö†Ô∏è No matches found in this chunk (or API error)."
                for r in results:
                    if r.get("score", 0) > 50:
                        scored_results.append(r)
                time.sleep(0.5)

            scored_results.sort(key=lambda x: x.get("score", 0), reverse=True)
            final_posts = [
                pmap[r["id"]] for r in scored_results[:target_posts] if r["id"] in pmap
            ]
            yield (
                f"   üéâ Approved {len(final_posts)} posts."
                if final_posts
                else "\n‚ÑπÔ∏è 0 matches approved by AI."
            )

        if final_posts:
            self.save_to_blacklist(final_posts, keywords, criteria)
        return final_posts

    def save_to_blacklist(self, posts: List, keywords: str, criteria: str):
        current_bl = {}
        if os.path.exists(BLACKLIST_FILE):
            try:
                with open(BLACKLIST_FILE, "r") as f:
                    current_bl = json.load(f)
            except:
                pass
        for p in posts:
            current_bl[p.id] = {
                "id": p.id,
                "title": p.title,
                "url": f"https://www.reddit.com{p.permalink}",
                "keywords": keywords,
                "criteria": criteria,
                "timestamp": time.time(),
            }
        with open(BLACKLIST_FILE, "w") as f:
            json.dump(current_bl, f, indent=2)

    def discover_subreddits(
        self, keywords: str, limit_subs: int = 10, criteria: str = ""
    ) -> Generator[str, None, List[Dict]]:
        parsed_groups, sub_hits, sub_details = self.parse_keywords(keywords), {}, {}
        yield "üåç Searching r/all..."
        simple_query = (
            " ".join([g[0] for g in parsed_groups]) if parsed_groups else keywords
        )
        try:
            for post in self.reddit.subreddit("all").search(
                simple_query, sort="new", limit=300
            ):
                if self.matches_logic(f"{post.title} {post.selftext}", parsed_groups):
                    s_name = post.subreddit.display_name
                    if s_name not in sub_hits:
                        sub_hits[s_name], sub_details[s_name] = 0, post.subreddit
                    sub_hits[s_name] += 1
            yield f"‚úÖ Found {len(sub_hits)} communities."
            sorted_subs = sorted(sub_hits.items(), key=lambda x: x[1], reverse=True)[
                : limit_subs * 2
            ]
            final_subs = []
            if not criteria:
                for name, hits in sorted_subs[:limit_subs]:
                    try:
                        desc = sub_details[name].public_description or ""
                        final_subs.append(
                            {"name": name, "hits": hits, "description": desc}
                        )
                    except:
                        pass
            else:
                yield "ü§ñ AI analyzing..."
                for name, hits in sorted_subs:
                    if len(final_subs) >= limit_subs:
                        break
                    try:
                        s_obj = sub_details[name]
                        desc, prompt_data = (
                            s_obj.public_description or "",
                            {
                                "subreddit": name,
                                "description": desc,
                                "match_count": hits,
                            },
                        )
                        if self.llm.analyze(
                            [prompt_data], f"Is relevant to: {criteria}?"
                        ):
                            final_subs.append(
                                {"name": name, "hits": hits, "description": desc}
                            )
                            yield f"   ‚úÖ AI Approved: r/{name}"
                        else:
                            yield f"   ‚ùå AI Rejected: r/{name}"
                    except:
                        pass
            return final_subs
        except Exception as e:
            yield f"‚ùå Error: {e}"
            return []


app = Flask(__name__)


@app.route("/")
def home():
    return render_template("index.html")


@app.route("/api/generate_query", methods=["POST"])
def api_generate_query():
    data = request.json
    desc, provider = data.get("description"), data.get("provider", "mistral")
    try:
        query_string = LLMFilter(provider).generate_boolean_string(desc)
        if query_string.startswith("Error:"):
            return jsonify({"error": query_string}), 500
        return jsonify({"query": query_string})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/api/blacklist", methods=["GET"])
def get_blacklist():
    if os.path.exists(BLACKLIST_FILE):
        try:
            with open(BLACKLIST_FILE, "r") as f:
                return jsonify(json.load(f))
        except:
            pass
    return jsonify({})


@app.route("/api/blacklist/clear", methods=["POST"])
def clear_blacklist():
    with open(BLACKLIST_FILE, "w") as f:
        json.dump({}, f)
    return jsonify({"status": "cleared"})


@app.route("/api/subreddits", methods=["GET"])
def get_subreddits():
    if os.path.exists(SUBREDDITS_FILE):
        try:
            with open(SUBREDDITS_FILE, "r") as f:
                return jsonify(json.load(f))
        except:
            pass
    return jsonify([s.strip() for s in DEFAULT_SUBS_ENV.split(",") if s.strip()])


@app.route("/api/subreddits", methods=["POST"])
def update_subreddits():
    with open(SUBREDDITS_FILE, "w") as f:
        json.dump(request.json, f, indent=2)
    return jsonify({"status": "saved"})


@app.route("/api/queries", methods=["GET"])
def get_queries():
    if not os.path.exists(SAVED_QUERIES_FILE):
        return jsonify({})
    try:
        with open(SAVED_QUERIES_FILE, "r") as f:
            return jsonify(json.load(f))
    except:
        return jsonify({})


@app.route("/api/queries", methods=["POST"])
def save_query_route():
    data = request.json
    name, current = data.get("name"), {}
    if os.path.exists(SAVED_QUERIES_FILE):
        try:
            with open(SAVED_QUERIES_FILE, "r") as f:
                current = json.load(f)
        except:
            pass
    current[name] = data["payload"]
    with open(SAVED_QUERIES_FILE, "w") as f:
        json.dump(current, f, indent=2)
    return jsonify({"status": "saved"})


@app.route("/api/queries/<name>", methods=["DELETE"])
def delete_query(name):
    if os.path.exists(SAVED_QUERIES_FILE):
        with open(SAVED_QUERIES_FILE, "r") as f:
            data = json.load(f)
        if name in data:
            del data[name]
            with open(SAVED_QUERIES_FILE, "w") as f:
                json.dump(data, f, indent=2)
    return jsonify({"status": "deleted"})


@app.route("/stream")
def stream():
    args = request.args
    subs_list = [
        s.strip() for s in args.get("subreddits", "").split(",") if s.strip()
    ] or ["ArtificialInteligence"]
    engine = RedditSearchEngine(
        provider=args.get("provider", "mistral"), debug=args.get("debug") == "true"
    )

    def generate():
        def send(msg):
            return f"data: {msg}\n\n"

        gen = engine.search(
            keywords=args.get("keywords", ""),
            criteria=args.get("criteria", ""),
            subreddits=subs_list,
            target_posts=int(args.get("target_posts", 10)),
            ai_chunk_size=int(args.get("ai_chunk", 5)),
            is_deep_scan=args.get("deep_scan") == "true",
            manual_limit=int(args.get("scan_limit", 100)),
            sort_by=args.get("sort", "new"),
            time_filter=args.get("time", "all"),
            post_type=args.get("post_type", "any"),
        )
        final_posts = []
        try:
            while True:
                yield send(next(gen))
        except StopIteration as e:
            final_posts = e.value

        html_res = "".join(
            [
                f"""<div class="card"><h3>{p.title}</h3><span class="meta">r/{p.subreddit.display_name} | <a href="https://www.reddit.com{p.permalink}" target="_blank">Open</a></span><hr style="border-color:#333"><div class="body-text">{p.selftext.replace("<", "&lt;").replace(">", "&gt;").replace("\n", "<br>")}</div></div>"""
                for p in final_posts
            ]
        )
        yield send(f"<<<HTML_RESULT>>>{html_res.replace('\n', '||NEWLINE||')}")

    return Response(generate(), mimetype="text/event-stream")


@app.route("/stream_discovery")
def stream_discovery():
    args = request.args
    engine = RedditSearchEngine(provider=args.get("provider", "mistral"))

    def generate():
        def send(msg):
            return f"data: {msg}\n\n"

        gen = engine.discover_subreddits(
            args.get("keywords", ""),
            int(args.get("limit", 10)),
            args.get("criteria", ""),
        )
        final_subs = []
        try:
            while True:
                yield send(next(gen))
        except StopIteration as e:
            final_subs = e.value
        yield send("‚ú® Rendering results...")
        for sub in final_subs:
            yield send(f"<<<JSON_CARD>>>{json.dumps(sub)}")
        yield send("<<<DONE>>>")

    return Response(generate(), mimetype="text/event-stream")


def run_cli():
    parser = argparse.ArgumentParser(description="Reddit Search Engine CLI")
    parser.add_argument("--keywords", type=str, help="Boolean search string")
    parser.add_argument("--criteria", type=str, help="AI filtering criteria")
    parser.add_argument("--subreddits", type=str, help="Comma-separated subreddits")
    parser.add_argument(
        "--target_posts", type=int, default=10, help="Target number of posts"
    )
    parser.add_argument(
        "--provider",
        type=str,
        default="mistral",
        choices=["mistral", "gemini"],
        help="LLM provider",
    )
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    parser.add_argument("--debug", action="store_true", help="Debug mode")
    parser.add_argument("--discovery", action="store_true", help="Run discovery mode")
    parser.add_argument(
        "--deep_scan", action="store_true", help="Enable deep historical scan"
    )
    parser.add_argument(
        "--sort",
        type=str,
        default="new",
        choices=["new", "relevance", "top", "hot", "comments"],
        help="Sort order",
    )
    parser.add_argument(
        "--time",
        type=str,
        default="all",
        choices=["all", "day", "hour", "month", "week", "year"],
        help="Time filter",
    )
    parser.add_argument(
        "--post_type",
        type=str,
        default="any",
        choices=["any", "text", "media"],
        help="Filter by post type",
    )
    parser.add_argument(
        "--limit", type=int, default=100, help="Manual fetch limit (max posts to scan)"
    )
    parser.add_argument(
        "--ai_chunk", type=int, default=5, help="Batch size for AI analysis"
    )
    args = parser.parse_args()

    if not args.keywords:
        parser.print_help()
        return

    engine = RedditSearchEngine(provider=args.provider, debug=args.debug)
    if args.discovery:
        gen = engine.discover_subreddits(
            args.keywords, limit_subs=args.target_posts, criteria=args.criteria or ""
        )
        final_results = []
        try:
            for msg in gen:
                if not args.json:
                    print(msg)
        except StopIteration as e:
            final_results = e.value
        if args.json:
            print(json.dumps(final_results, indent=2))
        else:
            print(f"\nFound {len(final_results)} subreddits.")
    else:
        subs = (
            [s.strip() for s in args.subreddits.split(",")]
            if args.subreddits
            else DEFAULT_SUBS_ENV.split(",")
        )
        gen = engine.search(
            keywords=args.keywords,
            criteria=args.criteria or "",
            subreddits=subs,
            target_posts=args.target_posts,
            ai_chunk_size=args.ai_chunk,
            is_deep_scan=args.deep_scan,
            manual_limit=args.limit,
            sort_by=args.sort,
            time_filter=args.time,
            post_type=args.post_type,
        )
        final_posts = []
        try:
            for msg in gen:
                if not args.json:
                    print(msg)
        except StopIteration as e:
            final_posts = e.value
        if args.json:
            output = [
                {
                    "id": p.id,
                    "title": p.title,
                    "subreddit": p.subreddit.display_name,
                    "url": f"https://www.reddit.com{p.permalink}",
                    "content": p.selftext,
                }
                for p in final_posts
            ]
            print(json.dumps(output, indent=2))
        else:
            print(f"\nFound {len(final_posts)} approved posts.")


if __name__ == "__main__":
    if len(sys.argv) > 1:
        run_cli()
    else:
        app.run(host="0.0.0.0", port=5000)
