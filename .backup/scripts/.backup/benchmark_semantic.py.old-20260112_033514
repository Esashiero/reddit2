#!/usr/bin/env python3
import sys
import os
import json
import argparse
import time
from typing import List, Dict
from dotenv import load_dotenv, find_dotenv
from mistralai import Mistral
import requests

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app import RedditSearchEngine

load_dotenv(find_dotenv(), override=True)


class SemanticBenchmark:
    def __init__(self, provider="mistral"):
        self.provider = provider
        self.engine = RedditSearchEngine(provider=provider, debug=False)
        self.mistral_key = os.getenv("MISTRAL_API_KEY")
        self.google_key = os.getenv("GOOGLE_API_KEY")

        if provider == "mistral" and self.mistral_key:
            self.client = Mistral(api_key=self.mistral_key)
        elif provider == "gemini" and not self.google_key:
            raise ValueError("Google API Key missing")

    def evaluate_relevance(self, user_request: str, post_content: str) -> int:
        sys_prompt = (
            "You are a relevance judge. "
            "Rate the relevance of the Reddit post to the User Request on a scale of 0-10. "
            "0 = Completely irrelevant/spam. "
            "10 = Perfect match, exactly what user asked for. "
            "Return ONLY the number."
        )
        user_prompt = (
            f"User Request: {user_request}\n\nReddit Post:\n{post_content[:2000]}"
        )

        try:
            if self.provider == "mistral":
                res = self.client.chat.complete(
                    model="mistral-small-latest",
                    messages=[
                        {"role": "system", "content": sys_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                )
                content = res.choices[0].message.content.strip()
                return int("".join(filter(str.isdigit, content)))
            else:
                url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={self.google_key}"
                payload = {
                    "contents": [{"parts": [{"text": user_prompt}]}],
                    "system_instruction": {"parts": [{"text": sys_prompt}]},
                }
                res = requests.post(
                    url, json=payload, headers={"Content-Type": "application/json"}
                )
                txt = res.json()["candidates"][0]["content"]["parts"][0]["text"].strip()
                return int("".join(filter(str.isdigit, txt)))
        except Exception as e:
            print(f"Evaluation error: {e}", file=sys.stderr)
            return 0

    def run_benchmark(self, description: str, subreddits: List[str], limit: int = 5):
        print(f"Benchmarking: {description}")

        llm_filter = self.engine.llm
        query = llm_filter.generate_boolean_string(description)
        print(f"Generated Query: {query}")

        if query.startswith("Error:"):
            print(f"‚ùå Aborting benchmark due to Query Generation Error: {query}")
            return {
                "description": description,
                "query": query,
                "found_count": 0,
                "average_score": 0,
                "details": [],
                "error": True,
            }

        print("Fetching posts from Reddit...")

        results = []
        try:
            gen = self.engine.search(
                keywords=query,
                criteria="",
                subreddits=subreddits,
                target_posts=limit,
                manual_limit=limit * 2,
            )
            for msg in gen:
                pass
            pass
        except StopIteration as e:
            results = e.value
        except Exception as e:
            print(f"Search failed: {e}")
            return

        print(f"Found {len(results)} posts.")

        total_score = 0
        details = []

        for post in results:
            score = self.evaluate_relevance(
                description, f"{post.title}\n{post.selftext}"
            )
            details.append({"title": post.title, "score": score, "url": post.permalink})
            total_score += score
            print(f"  [{score}/10] {post.title[:50]}...")

        avg_score = (total_score / len(results)) * 10 if results else 0
        print(f"Average Relevance: {avg_score:.1f}/100")
        return {
            "description": description,
            "query": query,
            "found_count": len(results),
            "average_score": avg_score,
            "details": details,
        }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--description", type=str, required=True)
    parser.add_argument("--subreddits", type=str, default="all")
    parser.add_argument("--limit", type=int, default=5)
    parser.add_argument("--provider", type=str, default="mistral")
    args = parser.parse_args()

    bench = SemanticBenchmark(provider=args.provider)
    subs = args.subreddits.split(",")

    bench.run_benchmark(args.description, subs, args.limit)


if __name__ == "__main__":
    main()
