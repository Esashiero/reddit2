Agent-Mediated Dialogic Inquiry: A Framework for Advanced Information Retrieval

1.0 Introduction: The Evolving Challenge of Information Retrieval

Information Retrieval (IR) systems are foundational to how we access knowledge, and the strategic refinement of user queries through query expansion is a critical component for improving search outcomes. While traditional techniques like Pseudo-Relevance Feedback (PRF) and early Large Language Model (LLM) based methods have advanced the field, they often struggle to capture the full complexity and nuance of user intent. These limitations prevent them from consistently delivering the most relevant information in an increasingly complex digital landscape.

Traditional query expansion methods, such as PRF, typically select terms from top-ranked documents to enrich the initial query. In contrast, recent LLM-based approaches‚Äîsuch as Q2D, which generates pseudo-documents; Q2C, which uses Chain-of-Thought prompting; and GenQREnsemble and GenQRFusion, which create and fuse keyword sets‚Äîleverage the generative power of models to create richer representations. Although these modern techniques represent a significant leap forward, they are constrained by several inherent challenges.

Existing LLM-based methods are constrained by three primary challenges:

* Homogeneous Expansions: Simplistic prompt variations frequently result in generated content that lacks contextual breadth and diversity, leading to narrow and repetitive query expansions.
* Lack of Dynamic Feedback: Most frameworks operate without a mechanism for iterative refinement. This absence of a feedback loop can result in the inclusion of redundant, suboptimal, or irrelevant content in the final query.
* Unstructured Reformulation: Expansions are often generated at the term or document level without a structured approach, limiting their ability to deconstruct and address the multifaceted nature of a user's true information need.

To address these shortcomings, we introduce the Agent-Mediated Dialogic (AMD) framework, a novel paradigm for query expansion. The core contribution of AMD is its use of a multi-agent, Socratic-inspired dialogic process. This collaborative inquiry among specialized AI agents produces richer, more diverse, and intent-aligned query representations that significantly enhance retrieval performance. The following section details the architectural components and dialogic workflow of the AMD framework.

2.0 The Agent-Mediated Dialogic (AMD) Framework Architecture

The strategic advantage of the AMD framework lies in its multi-agent architecture. By decomposing the complex task of query expansion into specialized roles‚Äîquestioning, answering, and feedback‚Äîthe system achieves a more robust and nuanced understanding of the user's initial query. This separation of concerns allows each agent to focus on a distinct aspect of the problem, leading to a more comprehensive and refined output than a monolithic approach could produce.

The overall workflow of the AMD framework is a structured dialogic interaction among three specialized agents. An initial user query is first reformulated into a set of diverse sub-questions. These questions then guide the generation of corresponding pseudo-answers, which are subsequently evaluated and refined by a feedback agent to ensure only the most relevant content contributes to the final, expanded query representation.

2.1 The Socratic Questioning Agent

The Socratic Questioning Agent initiates the dialogic process. Its function is inspired by studies showing that Socratic questioning enhances a model's reasoning and problem decomposition capabilities. This agent takes the initial user query and, in a single LLM inference, reformulates it into three diverse and targeted sub-questions. Each sub-question is designed to explore a distinct dimension of the user's intent, promoting a comprehensive and multi-faceted exploration of the information need. This process can be formalized as ùí¨ = Gùí¨(Qinit), where a set of sub-questions ùí¨ is generated from the initial query.

The three types of Socratic-inspired sub-questions are defined as follows:

Query Type	Role in Sub-question
Clarification	Crafts a sub-question to refine intent and ensure accurate interpretation of the user query.
Assumption Probing	Decomposes the query by surfacing implicit assumptions, adding diversity and depth to retrieval.
Implication Probing	Explores downstream effects to expand the query with relevant and diverse information.

2.2 The Dialogic Answering Agent

The Dialogic Answering Agent is responsible for the next stage of the interaction. For each of the three sub-questions generated by the Socratic Questioning Agent, this agent generates a corresponding pseudo-answer. These pseudo-answers function as surrogate documents, providing rich, contextual information that supplements the original query. This step is formalized as ùíú = Gùíú(ùí¨), where the set of pseudo-answers ùíú is generated in parallel.

This process is a deliberate dialogic exchange. Each answer is directly informed by the specific Socratic dimension‚Äîclarification, assumption, or implication‚Äîof its corresponding question. This ensures that the generated content is not only diverse but also precisely aligned with the multi-faceted intent uncovered by the questioning agent.

2.3 The Reflective Feedback Agent

The Reflective Feedback Agent performs the critical final step in the dialogic process. It assesses all three question-answer pairs collectively, viewing them in the context of the initial user query. This agent applies reflective reasoning to evaluate and selectively rewrite each pseudo-answer, systematically pruning content that is vague, redundant, or irrelevant to the core information need. The output is a refined set of pseudo-answers, formalized as ùíú' = G‚Ñ±({(qi, ai)}, Qinit).

By retaining only the most informative and intent-aligned content, this agent ensures the final query representation is both high-quality and concise. Notably, this agent is implemented using a non-finetuned LLM, a decision that demonstrates the generalizability and practical applicability of the AMD framework without requiring specialized model training. Having detailed the framework's architecture, we now turn to its empirical validation against state-of-the-art benchmarks.

3.0 Empirical Validation and Performance Analysis

Rigorous empirical validation is essential for establishing the practical value of any new framework. This section synthesizes the results from extensive experiments conducted on widely used benchmark datasets, providing a quantitative case for the Agent-Mediated Dialogic (AMD) framework's superiority over existing state-of-the-art query expansion methods.

The experimental setup was designed for comprehensive evaluation. The AMD framework was tested on a total of eight benchmark datasets: six from the heterogeneous BEIR Benchmark (specifically Webis-Touche2020, SciFact, Trec-COVID-BEIR, DBPedia-Entity, SCIDOCS, and FIQA) and two from the challenging TREC Deep Learning tracks (2019 and 2020). For all experiments involving the AMD framework, the open-source Qwen2.5-7B-Instruct model was employed to demonstrate the framework's effectiveness with accessible, high-performance LLMs.

3.1 Comparative Performance: Sparse Retrieval

In sparse retrieval, where systems rely on lexical matching techniques like BM25, the quality of query expansion is paramount. The AMD framework's ability to generate rich, contextually relevant pseudo-answers provides a significant advantage.

Method	BEIR Avg. Score (nDCG@10)	TREC DL'19 (nDCG@10)	TREC DL'20 (nDCG@10)
BM25	0.3672	0.4758	0.4500
Q2D	0.4193	0.5486	0.5222
AMD (Sparse, Ours)	0.4352	0.5818	0.5433

The results demonstrate AMD's superiority over the Q2D baseline. While Q2D generates pseudo-documents to reflect information needs, it lacks an effective filtering mechanism for less informative content. In contrast, AMD‚Äôs architecture, particularly the Reflective Feedback Agent, prunes irrelevant content to produce a more precise and potent query representation, leading to notable improvements across all sparse retrieval benchmarks.

3.2 Comparative Performance: Dense Retrieval

For dense retrieval, which relies on semantic understanding via embeddings, the AMD framework's multi-faceted query enrichment proves highly effective. The dialogic process captures deeper contextual relationships, leading to more precise vector representations.

Method	BEIR Avg. Score (nDCG@10)	TREC DL'19 (nDCG@10)	TREC DL'20 (nDCG@10)
E5-Base	0.4324	0.7029	0.6339
Q2D	0.4560	0.6971	0.6385
AMD (Dense, Ours)	0.4707	0.7128	0.6488

As with sparse retrieval, the analysis shows that AMD achieves higher performance than both the baseline dense retriever (E5-Base) and the Q2D expansion method. This confirms that AMD's structured and filtered expansions translate into significant gains in dense, embedding-based retrieval scenarios by providing richer semantic context without introducing noise.

3.3 Comparative Performance: Reciprocal Rank Fusion (RRF)

Reciprocal Rank Fusion (RRF) aggregates rankings from multiple queries to produce a final, more robust result. AMD's approach, which generates three high-quality, diverse expansions, is naturally suited for this strategy.

Method	BEIR Avg. Score (nDCG@10)	TREC DL'19 (nDCG@10)	TREC DL'20 (nDCG@10)
GenQRFusion	0.3897	0.4375	0.4515
AMD (RRF, Ours)	0.4113	0.5325	0.5077

The comparison reveals that AMD's strategy, which uses three diverse, high-quality expansions refined by the feedback agent, delivers superior results compared to GenQRFusion, which relies on up to ten un-filtered LLM inferences. This highlights AMD's ability to achieve better performance with significantly less computational overhead, making it a more efficient and effective solution.

3.4 Ablation Study: The Impact of the Reflective Feedback Agent

To isolate and quantify the contribution of the framework's final stage, an ablation study was conducted to assess the effectiveness of the Reflective Feedback Agent. This study compared the full AMD framework against a variant where the feedback and rewriting step was removed.

The findings confirm that the inclusion of the Reflective Feedback Agent consistently improves the overall average score across both sparse and dense retrieval on the BEIR and TREC datasets. This result underscores the agent's critical role in enhancing the framework's robustness; without it, performance variability increases and more noise from less relevant pseudo-answers is observed. By filtering this noise, the agent enhances the quality of the final query representation.

Significantly, the analysis also reveals that even the AMD variant without the feedback module achieves a higher average score than other baseline methods. This demonstrates the inherent strength of the Socratic questioning and dialogic answering stages. However, the full framework, with its dynamic feedback mechanism, consistently delivers the most stable and superior retrieval performance.

4.0 Integration and Retrieval Strategies

A key strength of the Agent-Mediated Dialogic framework is its flexibility. AMD is designed to be compatible with multiple retrieval methods, allowing it to be adapted to a wide variety of existing IR systems and architectures. The framework's refined pseudo-answers can be integrated with the initial query using several distinct aggregation strategies, each tailored to a specific retrieval modality.

4.1 Sparse Query Aggregation

For sparse retrieval systems that rely on lexical matching (e.g., BM25), the refined pseudo-answers are concatenated directly with the initial query. In this method, the initial query is first replicated three times to reinforce its core signals. Then, all three refined pseudo-answers are appended, with [SEP] tokens used as separators to distinguish between the different components. This creates a single, enriched query string for the retrieval engine.

4.2 Dense Query Aggregation

In the context of dense retrieval, where queries are represented as high-dimensional vectors, the final query embedding is computed as a weighted combination. Following previous work (Seo et al., 2024), a weight of 0.7 is assigned to the initial query embedding to preserve the original user intent, while the remaining 0.3 weight is given to the averaged embedding of the three refined pseudo-answers, which provides semantic enrichment.

4.3 Reciprocal Rank Fusion (RRF) Aggregation

For integration with Reciprocal Rank Fusion (RRF), each of the three refined pseudo-answers is treated as an individual expanded query. The retrieval system executes a separate search for each of these queries, generating three distinct ranked lists of documents. The final document scores are then computed by aggregating these rankings, using a constant k=60 to dampen the influence of lower-ranked documents. This approach leverages the diversity of the generated expansions to produce a more robust final ranking. These flexible integration strategies underscore the framework's adaptability, a key contribution summarized in the concluding section.

5.0 Conclusion and Future Directions

This paper introduced the Agent-Mediated Dialogic (AMD) framework, a novel agent-based paradigm for query expansion. AMD combines three specialized agents‚Äîa Socratic Questioning Agent, a Dialogic Answering Agent, and a Reflective Feedback Agent‚Äîto transform an initial query into a robust, intent-aligned representation. By reformulating queries into diverse sub-questions and iteratively refining the resulting pseudo-answers, the framework produces multi-faceted augmentations that capture the complexity of user intent more effectively than previous methods.

The primary conclusion from our experiments is clear and compelling. Comprehensive validation on eight widely used benchmark datasets confirms that the structured agent collaboration within the AMD framework leads to consistent and significant improvements in retrieval performance across sparse, dense, and fusion-based methods.

Despite its strong performance, the current framework has limitations. Because the Reflective Feedback Agent is based on a non-finetuned model, some residual noise may persist in the expanded queries. The agent is effective at filtering out a significant amount of irrelevant or redundant content, but it may not achieve perfect selectivity in all cases.

To address this, future work will focus on enhancing the framework's refinement capabilities. We plan to explore fine-tuning the feedback agent on domain-specific data to further reduce noise and improve its ability to select only the most valuable content for expansion. Additionally, a more detailed qualitative analysis of each agent's output will be conducted to offer deeper insights into their respective contributions and identify further opportunities for optimization.
