Leveraging insights from 'LLM-as-a-Judge' and 'Evaluating search relevance part 2 - Phi-3 as relevance judge,' how can we enhance the _evaluate_relevance LLM prompt to identify specific reasons for low relevance (e.g., 'not first-person,' 'not about brother,' 'not during sleep') instead of just a score? This structured feedback could be invaluable for the optimization expert."
*   "Can the relevance judge identify if a post is a meta-discussion about the topic rather than a direct story, and penalize it accordingly?

To enhance the _evaluate_relevance LLM prompt and incorporate structured, reason-based feedback while specifically penalizing "meta-discussions," you can leverage several key strategies from the sources:
1. Transition to a "Rationale-First" Scoring Model
Both the Phi-3 case study and the "LLM-as-a-Judge" research indicate that forcing an LLM to provide a justification before the numerical score significantly improves alignment with human judgment
.

• Modification: Update the prompt to require a Chain-of-Thought (CoT) reasoning path. The sources suggest a specific four-step structure:
    1. Infer Intent: Expand the user request to understand implicit goals
.
    2. Summarize Content: Extract key attributes from the Reddit post
.
    3. Identify Mismatches: Explicitly look for reasons the post might fail (e.g., "not first-person")
.
    4. Final Score: Assign the 0–10 score based on the accumulated evidence
.

2. Implement Structured Feedback via JSON Keys
To provide the "Search Optimization Expert" with actionable data, the _evaluate_relevance method should return more than just a digit. You can modify the LLMFilter logic to output a JSON object containing mismatch attributes
.

Proposed Enhanced Output Format:

{
  "relevance_score": 3,
  "mismatch_reasons": ["not first-person", "not about brother", "meta-discussion"],
  "rationale": "The user asked for a direct story about a brother, but the post is a community discussion about why 'brother stories' are popular.",
  "is_meta_discussion": true
}

• This structured feedback allows the Optimization Expert to distinguish between an "over-constrained" query (0 results) and a "topic drift" issue where keywords match but the context is wrong
.

3. Detecting and Penalizing Meta-Discussions
The relevance judge can be explicitly instructed to evaluate semantic fidelity and intent alignment rather than just keyword presence
.

• Instructional Layer: Add a specific criterion for "Contextual Appropriateness"
. Define a "meta-discussion" as any post that discusses the topic as a concept rather than providing a direct instance or story
.

• Penalization Logic: Instruct the judge that if is_meta_discussion is true, the score cannot exceed a certain threshold (e.g., 4/10), even if all keywords match
. This prevents the system from "converging" on popular but useless community meta-posts
.

4. Dynamic Prompt Routing for Fine-Grained Accuracy
The "LLM-as-a-Judge" research introduces Contextual Evaluation Prompt Routing, which dynamically adjusts the evaluation rubric based on the category of the query
.

• Application: If the user description is identified as a "Personal Story," the system should automatically route to a prompt that includes strict criteria for "First-Person Perspective" and "Direct Experience"
. This reduces hallucinations by focusing the LLM on specific constraints relevant only to that domain
.

5. Leveraging Self-Critique for Iterative Improvement
By using a Self-Critique Guided Iterative Reasoning (SiGIR) framework, the judge doesn't just score the post—it generates a "self-critique reward" for the current search iteration
.

• If the judge identifies a trend of "meta-discussions," it can pass a "negative evidence" signal back to the Optimization Expert
.

• The Expert then uses this to refine the manual_query for the next pass, adding exclusionary Lucene terms such as NOT ("discussion" OR "meta" OR "question") to filter out non-story content
.


-------------------------------------------------------------------------------
Analogy for the Enhanced Judge: Instead of a simple Pass/Fail sensor at a factory gate, your enhanced relevance judge acts like a detailed quality inspector. The inspector doesn't just reject a part; they fill out a form stating "the screw is too long" or "the metal is the wrong alloy." This form (the structured JSON) is then sent back to the machine operator (the Optimization Expert) who uses it to recalibrate the machine (the Boolean query) for a perfect batch.